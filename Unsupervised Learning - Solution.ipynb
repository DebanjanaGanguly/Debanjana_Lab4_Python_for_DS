{"cells":[{"cell_type":"markdown","id":"83f26a29","metadata":{"id":"83f26a29"},"source":["# Unsupervised Lab Session"]},{"cell_type":"markdown","id":"8ea571d1","metadata":{"id":"8ea571d1"},"source":["## Learning outcomes:\n","- Exploratory data analysis and data preparation for model building.\n","- PCA for dimensionality reduction.\n","- K-means and Agglomerative Clustering"]},{"cell_type":"markdown","id":"fd7f778a","metadata":{"id":"fd7f778a"},"source":["## Problem Statement\n","Based on the given marketing campigan dataset, segment the similar customers into suitable clusters. Analyze the clusters and provide your insights to help the organization promote their business."]},{"cell_type":"markdown","id":"33b58f8f","metadata":{"id":"33b58f8f"},"source":["## Context:\n","- Customer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n","- Customer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment."]},{"cell_type":"markdown","id":"867166aa","metadata":{"id":"867166aa"},"source":["## About dataset\n","- Source: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis?datasetId=1546318&sortBy=voteCount\n","\n","### Attribute Information:\n","- ID: Customer's unique identifier\n","- Year_Birth: Customer's birth year\n","- Education: Customer's education level\n","- Marital_Status: Customer's marital status\n","- Income: Customer's yearly household income\n","- Kidhome: Number of children in customer's household\n","- Teenhome: Number of teenagers in customer's household\n","- Dt_Customer: Date of customer's enrollment with the company\n","- Recency: Number of days since customer's last purchase\n","- Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n","- MntWines: Amount spent on wine in last 2 years\n","- MntFruits: Amount spent on fruits in last 2 years\n","- MntMeatProducts: Amount spent on meat in last 2 years\n","- MntFishProducts: Amount spent on fish in last 2 years\n","- MntSweetProducts: Amount spent on sweets in last 2 years\n","- MntGoldProds: Amount spent on gold in last 2 years\n","- NumDealsPurchases: Number of purchases made with a discount\n","- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n","- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n","- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n","- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n","- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n","- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n","- NumWebPurchases: Number of purchases made through the company’s website\n","- NumCatalogPurchases: Number of purchases made using a catalogue\n","- NumStorePurchases: Number of purchases made directly in stores\n","- NumWebVisitsMonth: Number of visits to company’s website in the last month"]},{"cell_type":"markdown","id":"5a830406","metadata":{"id":"5a830406"},"source":["### 1. Import required libraries"]},{"cell_type":"code","execution_count":null,"id":"d65c5528","metadata":{"id":"d65c5528"},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn\n","from sklearn.cluster import KMeans\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score,confusion_matrix\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.cluster import AgglomerativeClustering\n","import scipy.cluster.hierarchy as sch\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.decomposition import PCA\n"]},{"cell_type":"markdown","id":"c80eb960","metadata":{"id":"c80eb960"},"source":["### 2. Load the CSV file (i.e marketing.csv) and display the first 5 rows of the dataframe. Check the shape and info of the dataset."]},{"cell_type":"code","execution_count":null,"id":"1caebc10","metadata":{"id":"1caebc10"},"outputs":[],"source":["df=pd.read_csv('\"C:/Users/DEBANJANA/OneDrive/Desktop/marketing.csv\"')\n","df.head()"]},{"cell_type":"markdown","id":"9ef75724","metadata":{"id":"9ef75724"},"source":["### 3. Check the percentage of missing values? If there is presence of missing values, treat them accordingly."]},{"cell_type":"code","execution_count":null,"id":"f2c231df","metadata":{"id":"f2c231df"},"outputs":[],"source":["df.isnull().sum()/len(df)*100"]},{"cell_type":"markdown","id":"86f3709e","metadata":{"id":"86f3709e"},"source":["### 4. Check if there are any duplicate records in the dataset? If any drop them."]},{"cell_type":"code","execution_count":null,"id":"2970671a","metadata":{"id":"2970671a"},"outputs":[],"source":["duplicates = df[df.duplicated()]\n","df.drop_duplicates(inplace=True)"]},{"cell_type":"markdown","id":"3a6f2b5a","metadata":{"id":"3a6f2b5a"},"source":["### 5. Drop the columns which you think redundant for the analysis "]},{"cell_type":"code","execution_count":null,"id":"a9ca818b","metadata":{"id":"a9ca818b"},"outputs":[],"source":["df.drop(columns='B', inplace=True)"]},{"cell_type":"markdown","id":"4ff0a112","metadata":{"id":"4ff0a112"},"source":["### 6. Check the unique categories in the column 'Marital_Status'\n","- i) Group categories 'Married', 'Together' as 'relationship'\n","- ii) Group categories 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' as 'Single'."]},{"cell_type":"code","execution_count":null,"id":"eb1be519","metadata":{"id":"eb1be519"},"outputs":[],"source":["def group_marital_status(status):\n","    if status in ['Married', 'Together']:\n","        return 'relationship'\n","    elif status in ['Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd']:\n","        return 'Single'\n","\n","df['Marital_Status_Grouped'] = df['Marital_Status'].apply(group_marital_status)\n","unique_categories = df['Marital_Status_Grouped'].unique()\n","print(\"Unique categories in 'Marital_Status_Grouped':\", unique_categories)"]},{"cell_type":"markdown","id":"9566bfbe","metadata":{"id":"9566bfbe"},"source":["### 7. Group the columns 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', and 'MntGoldProds' as 'Total_Expenses'"]},{"cell_type":"code","execution_count":null,"id":"3c3fa800","metadata":{"id":"3c3fa800"},"outputs":[],"source":["columns_to_sum1 = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n","\n","df['Total_Expenses'] = df[columns_to_sum1].sum(axis=1)\n"]},{"cell_type":"markdown","id":"bf0cd083","metadata":{"id":"bf0cd083"},"source":["### 8. Group the columns 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', and 'NumDealsPurchases' as 'Num_Total_Purchases'"]},{"cell_type":"code","execution_count":null,"id":"9c535ede","metadata":{"id":"9c535ede"},"outputs":[],"source":["columns_to_sum2 = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases']\n","\n","df['Num_Total_Purchases'] = df[columns_to_sum2].sum(axis=1)"]},{"cell_type":"markdown","id":"52d2dca5","metadata":{"id":"52d2dca5"},"source":["### 9. Group the columns 'Kidhome' and 'Teenhome' as 'Kids'"]},{"cell_type":"code","execution_count":null,"id":"f7c861a1","metadata":{"id":"f7c861a1"},"outputs":[],"source":["columns_to_sum3 = ['Kidhome', 'Teenhome']\n","\n","df['Kids'] = df[columns_to_sum3].sum(axis=1)"]},{"cell_type":"markdown","id":"36f67474","metadata":{"id":"36f67474"},"source":["### 10. Group columns 'AcceptedCmp1 , 2 , 3 , 4, 5' and 'Response' as 'TotalAcceptedCmp'"]},{"cell_type":"code","execution_count":null,"id":"ecc9109f","metadata":{"id":"ecc9109f"},"outputs":[],"source":["columns_to_sum4 = ['AcceptedCmp1 , 2 , 3 , 4, 5', 'Response']\n","\n","df['TotalAcceptedCmp'] = df[columns_to_sum4].sum(axis=1)"]},{"cell_type":"markdown","id":"886bfb08","metadata":{"id":"886bfb08"},"source":["### 11. Drop those columns which we have used above for obtaining new features"]},{"cell_type":"code","execution_count":null,"id":"e853e663","metadata":{"id":"e853e663"},"outputs":[],"source":["columns_to_drop = ['Married', 'Together', 'Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd', 'ines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases', 'Kidhome', 'Teenhome', 'AcceptedCmp1 , 2 , 3 , 4, 5', 'Response']\n","df = df.drop(columns=columns_to_drop)\n"]},{"cell_type":"markdown","id":"4225ced7","metadata":{"id":"4225ced7"},"source":["### 12. Extract 'age' using the column 'Year_Birth' and then drop the column 'Year_birth'"]},{"cell_type":"code","execution_count":null,"id":"d517611e","metadata":{"id":"d517611e"},"outputs":[],"source":["df['Age'] = 2024 - df['Year_Birth']\n","df = df.drop(columns=['Year_Birth'])\n"]},{"cell_type":"markdown","id":"f2d3c92d","metadata":{"id":"f2d3c92d"},"source":["### 13. Encode the categorical variables in the dataset"]},{"cell_type":"code","execution_count":null,"id":"030cfc32","metadata":{"id":"030cfc32"},"outputs":[],"source":["label_encoder = LabelEncoder()\n","df['Education_Label'] = label_encoder.fit_transform(df['Education'])\n","df['Marital_Status_Label'] = label_encoder.fit_transform(df['Marital_Status_Grouped'])\n"]},{"cell_type":"markdown","id":"9242e36d","metadata":{"id":"9242e36d"},"source":["### 14. Standardize the columns, so that values are in a particular range"]},{"cell_type":"code","execution_count":null,"id":"72475b68","metadata":{"id":"72475b68"},"outputs":[],"source":["columns_to_standardize = [\n","    'relationship', 'Single', 'Total_Expenses', 'Num_Total_Purchases', 'Kids', 'TotalAcceptedCmp'\n","]\n","\n","scaler = StandardScaler()\n","\n","df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","print(df.head())\n"]},{"cell_type":"markdown","id":"d063d2e2","metadata":{"id":"d063d2e2"},"source":["### 15. Apply PCA on the above dataset and determine the number of PCA components to be used so that 90-95% of the variance in data is explained by the same."]},{"cell_type":"code","execution_count":null,"id":"6df3c70e","metadata":{"id":"6df3c70e"},"outputs":[],"source":["onehot_encoder = OneHotEncoder(sparse=False)\n","\n","education_encoded = onehot_encoder.fit_transform(df[['Education']])\n","education_df = pd.DataFrame(education_encoded, columns=onehot_encoder.categories_[0])\n","df = pd.concat([df, education_df], axis=1)\n","\n","marital_status_encoded = onehot_encoder.fit_transform(df[['Marital_Status']])\n","marital_status_df = pd.DataFrame(marital_status_encoded, columns=onehot_encoder.categories_[0])\n","df = pd.concat([df, marital_status_df], axis=1)\n","\n","df.drop(['Married', 'Together', 'Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd', 'ines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases', 'Kidhome', 'Teenhome', 'AcceptedCmp1 , 2 , 3 , 4, 5', 'Response'], axis=1, inplace=True)\n","\n","columns_to_standardize = [\n","    'relationship', 'Single', 'Total_Expenses', 'Num_Total_Purchases', 'Kids', 'TotalAcceptedCmp'\n","]\n","\n","scaler = StandardScaler()\n","\n","df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","pca = PCA()\n","\n","pca.fit(df.drop('ID', axis=1)) \n","\n","explained_variance_ratio = pca.explained_variance_ratio_\n","cumulative_variance = explained_variance_ratio.cumsum()\n","\n","num_components_90 = next(i for i, total in enumerate(cumulative_variance) if total >= 0.90) + 1\n","num_components_95 = next(i for i, total in enumerate(cumulative_variance) if total >= 0.95) + 1\n","\n","print(f\"Number of components to explain at least 90% of the variance: {num_components_90}\")\n","print(f\"Number of components to explain at least 95% of the variance: {num_components_95}\")"]},{"cell_type":"markdown","id":"b2df19d7","metadata":{"id":"b2df19d7"},"source":["### 16. Apply K-means clustering and segment the data (Use PCA transformed data for clustering)"]},{"cell_type":"code","execution_count":null,"id":"a3a8bb4c","metadata":{"id":"a3a8bb4c"},"outputs":[],"source":["# Encoding 'Education' and 'Marital_Status' using One-Hot Encoding\n","onehot_encoder = OneHotEncoder(sparse=False)\n","\n","education_encoded = onehot_encoder.fit_transform(df[['Education']])\n","education_df = pd.DataFrame(education_encoded, columns=onehot_encoder.categories_[0])\n","df = pd.concat([df, education_df], axis=1)\n","\n","marital_status_encoded = onehot_encoder.fit_transform(df[['Marital_Status']])\n","marital_status_df = pd.DataFrame(marital_status_encoded, columns=onehot_encoder.categories_[0])\n","df = pd.concat([df, marital_status_df], axis=1)\n","\n","# Dropping original categorical columns\n","df.drop(['Married', 'Together', 'Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd', 'ines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases', 'Kidhome', 'Teenhome', 'AcceptedCmp1 , 2 , 3 , 4, 5', 'Response'], axis=1, inplace=True)\n","\n","# Select the columns to be standardized\n","columns_to_standardize = [\n","    'relationship', 'Single', 'Total_Expenses', 'Num_Total_Purchases', 'Kids', 'TotalAcceptedCmp'\n","]\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Standardize the columns\n","df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","\n","4. Applying PCA\n","\n","# Initialize PCA\n","pca = PCA()\n","\n","# Fit PCA on the standardized data\n","pca.fit(df.drop('ID', axis=1))  # Exclude 'ID' from PCA\n","\n","# Determine the number of components to explain at least 90% of the variance\n","explained_variance_ratio = pca.explained_variance_ratio_\n","cumulative_variance = explained_variance_ratio.cumsum()\n","num_components = next(i for i, total in enumerate(cumulative_variance) if total >= 0.90) + 1\n","\n","# Apply PCA with the determined number of components\n","pca = PCA(n_components=num_components)\n","pca_transformed = pca.fit_transform(df.drop('ID', axis=1))\n","\n","\n","5. Applying K-Means Clustering\n","\n","# Determine the optimal number of clusters using the elbow method\n","wcss = []\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters=i, random_state=42)\n","    kmeans.fit(pca_transformed)\n","    wcss.append(kmeans.inertia_)\n","\n","# Plot the elbow method\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n","plt.title('Elbow Method')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()\n","\n","# From the plot, choose an optimal number of clusters (e.g., 3 for demonstration)\n","optimal_clusters = 3\n","\n","# Apply K-Means with the optimal number of clusters\n","kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n","df['Cluster'] = kmeans.fit_predict(pca_transformed)\n","\n","# Show the resulting clustered data\n","print(df.head())"]},{"cell_type":"markdown","id":"d8463aed","metadata":{"id":"d8463aed"},"source":["### 17. Apply Agglomerative clustering and segment the data (Use Original data for clustering), and perform cluster analysis by doing bivariate analysis between the cluster label and different features and write your observations."]},{"cell_type":"code","execution_count":null,"id":"b5ca165b","metadata":{"id":"b5ca165b"},"outputs":[],"source":["columns_to_standardize = [\n","   'relationship', 'Single', 'Total_Expenses', 'Num_Total_Purchases', 'Kids', 'TotalAcceptedCmp'\n","]\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Standardize the columns\n","df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","agg_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n","df['Cluster'] = agg_cluster.fit_predict(df.drop('ID', axis=1))\n","\n","# Show the resulting clustered data\n","print(df.head())\n","\n","# Bivariate analysis using pairplot\n","sns.pairplot(df, hue='Cluster', vars=columns_to_standardize[:4])  # Pairplot for the first few features\n","plt.show()\n","\n","# Analyzing the distribution of features across clusters\n","for column in columns_to_standardize:\n","    plt.figure(figsize=(10, 5))\n","    sns.boxplot(x='Cluster', y=column, data=df)\n","    plt.title(f'Cluster vs {column}')\n","    plt.show()\n","\n","\n"]},{"cell_type":"markdown","id":"797a5ecd","metadata":{"id":"797a5ecd"},"source":["### Visualization and Interpretation of results"]},{"cell_type":"code","execution_count":null,"id":"d1e75760","metadata":{"id":"d1e75760"},"outputs":[],"source":["columns_to_standardize = [\n","'relationship', 'Single', 'Total_Expenses', 'Num_Total_Purchases', 'Kids', 'TotalAcceptedCmp'\n","]\n","for column in columns_to_standardize:\n","    plt.figure(figsize=(10, 5))\n","    sns.boxplot(x='Cluster', y=column, data=df)\n","    plt.title(f'Cluster vs {column}')\n","    plt.show()"]},{"cell_type":"markdown","id":"36afd95b","metadata":{"id":"36afd95b"},"source":["-----\n","## Happy Learning\n","-----"]}],"metadata":{"colab":{"collapsed_sections":["36afd95b"],"name":"Unsupervised Learning - Lab session.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}
